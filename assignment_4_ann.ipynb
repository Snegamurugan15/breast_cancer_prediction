# Import required libraries
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import pickle

# Step 1: Load and Prepare the Breast Cancer Dataset
def load_and_prepare_data():
    """
    Load the breast cancer dataset and prepare it for feature selection and training.

    Returns:
        df: Full dataset as a pandas DataFrame
        X: Features as a NumPy array
        y: Target as a NumPy array
        feature_names: Feature names
        target_names: Target names
    """
    data = load_breast_cancer()
    df = pd.DataFrame(data.data, columns=data.feature_names)  # Create DataFrame with feature data
    df['target'] = data.target  # Add target column
    feature_names = data.feature_names
    target_names = data.target_names

    # Data overview
    print("Dataset Head:\n", df.head())
    print("\nDataset Info:")
    print(df.info())

    # Check for missing values
    if df.isnull().sum().sum() == 0:
        print("\nNo missing values in the dataset.")
    else:
        print("\nMissing values found. Consider handling them.")

    X = df.drop(columns=['target']).values  # Features
    y = df['target'].values  # Target

    return df, X, y, feature_names, target_names

# Step 2: Feature Selection
def select_features(X, y, k=10):
    """
    Select the top k features using SelectKBest.

    Args:
        X: Feature dataset
        y: Target dataset
        k: Number of top features to select

    Returns:
        X_selected: Transformed feature set with selected features
        selected_features: List of selected feature names
    """
    selector = SelectKBest(score_func=f_classif, k=k)
    X_selected = selector.fit_transform(X, y)  # Apply feature selection
    selected_features = np.array(feature_names)[selector.get_support()]  # Get selected feature names

    print(f"\nTop {k} Selected Features:")
    for feature in selected_features:
        print(feature)

    return X_selected, selected_features

# Step 3: Data Splitting and Scaling
def split_and_scale_data(X, y, scaler_file="scaler.pkl"):
    """
    Split the dataset into training and testing sets, and scale the features.
    Saves the scaler object to a pickle file for later use.

    Args:
        X (array-like): Features dataset
        y (array-like): Target labels
        scaler_file (str): File path to save the scaler

    Returns:
        X_train, X_test, y_train, y_test (arrays): Scaled training and testing sets
    """
    # Split into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Scale the features
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Save the scaler to a pickle file
    with open(scaler_file, "wb") as file:
        pickle.dump(scaler, file)
    print(f"Scaler saved to {scaler_file}")

    return X_train, X_test, y_train, y_test

# Step 4: Create and Train the ANN Model
def train_ann_model(X_train, y_train, hidden_layer_sizes=(50,), activation='relu', solver='adam', max_iter=500, model_file="ann_model.pkl"):
    """
    Train an Artificial Neural Network (ANN) model and save it to a pickle file.

    Args:
        X_train (array-like): Training feature set
        y_train (array-like): Training target set
        hidden_layer_sizes (tuple): Hidden layer configuration
        activation (str): Activation function ('relu', 'tanh', etc.)
        solver (str): Optimization algorithm ('adam', 'sgd', etc.)
        max_iter (int): Maximum number of iterations for optimization
        model_file (str): File path to save the trained model

    Returns:
        model: Trained MLPClassifier model
    """
    # Initialize and train the model
    model = MLPClassifier(
        hidden_layer_sizes=hidden_layer_sizes,
        activation=activation,
        solver=solver,
        max_iter=max_iter,
        random_state=42
    )
    model.fit(X_train, y_train)  # Train the model

    print("\nModel Training Complete!")

    # Save the trained model to a pickle file
    with open(model_file, "wb") as file:
        pickle.dump(model, file)
    print(f"Model saved to {model_file}")

    return model

# Step 5: Model Evaluation
def evaluate_model(model, X_test, y_test, target_names):
    """
    Evaluate the trained model using accuracy, classification report, and confusion matrix.

    Args:
        model: Trained ANN model
        X_test: Test feature set
        y_test: Test target set
        target_names: Names of target classes
    """
    # Make predictions
    y_pred = model.predict(X_test)

    # Accuracy score
    accuracy = accuracy_score(y_test, y_pred)
    print(f"\nModel Accuracy: {accuracy:.2f}")

    # Classification report
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=target_names))

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    print("\nConfusion Matrix:")
    print(cm)

    # Plot confusion matrix
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)
    plt.title("Confusion Matrix")
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.show()

# Step 6: Main Pipeline
df, X, y, feature_names, target_names = load_and_prepare_data()
X_selected, selected_features = select_features(X, y, k=10)
X_train, X_test, y_train, y_test = split_and_scale_data(X_selected, y, scaler_file="scaler.pkl")
ann_model = train_ann_model(X_train, y_train, hidden_layer_sizes=(50, 50), activation='relu', solver='adam', max_iter=500, model_file="ann_model.pkl")
evaluate_model(ann_model, X_test, y_test, target_names)
